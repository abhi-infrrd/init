{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.sparse as sps\n",
    "import codecs\n",
    "from scipy.sparse import identity,coo_matrix, hstack\n",
    "from collections import Counter\n",
    "from hybrid_recommender import train_hybrid_recommender\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unique fields in a column of pandas dataframe\n",
    "def process_and_find_unique_fields(dataframe, column):\n",
    "    column_uniques = dataframe[column].unique()\n",
    "    unique_fields = []\n",
    "    for i in range(0, len(column_uniques)):\n",
    "        unique_candidate = column_uniques[i]\n",
    "        if unique_candidate != np.nan and unique_candidate != 'None':\n",
    "            unique_candidate = str(unique_candidate).replace(' ', '')\n",
    "            unique_candidate = str(unique_candidate).replace('\\\\', ',')\n",
    "            unique_candidate = str(unique_candidate).replace(';', ',')\n",
    "            fields = str(unique_candidate).split(',')\n",
    "            for j in range(0, len(fields)):\n",
    "                if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "                    unique_fields.append(fields[j].lower())\n",
    "    unique_fields = list(set(unique_fields))\n",
    "    return unique_fields\n",
    "\n",
    "# Finds unique fields in CPA_state column of pandas dataframe\n",
    "def process_and_find_unique_states(dataframe, column):\n",
    "    column_uniques = dataframe[column].unique()\n",
    "    unique_fields = []\n",
    "    for i in range(0, len(column_uniques)):\n",
    "        unique_candidate = column_uniques[i]\n",
    "        if unique_candidate != np.nan and unique_candidate != 'None':\n",
    "            unique_candidate = str(unique_candidate).replace(' ', '')\n",
    "            unique_candidate = str(unique_candidate).replace('\\\\', ',')\n",
    "            fields = str(unique_candidate).split(',')\n",
    "            for j in range(0, len(fields)):\n",
    "                if fields[j] !='' and fields[j] !='None'and fields[j] !='nan' and (not ('.' in fields[j])):\n",
    "                    unique_fields.append(fields[j].lower())\n",
    "    unique_fields = list(set(unique_fields))\n",
    "    return unique_fields\n",
    "\n",
    "# Finds top N keywords in column of pandas dataframe\n",
    "def get_top_Keywords(keyword_unique, N):\n",
    "    keyword_set = []\n",
    "    for topics in keyword_unique:\n",
    "        topic = str(topics).split(', ')\n",
    "        for top in topic:\n",
    "            if top != '':\n",
    "                keyword_set.append(str(top))\n",
    "    count = Counter(keyword_set)\n",
    "    common_list = count.most_common(N)\n",
    "    keyword_list = [ seq[0] for seq in common_list ]\n",
    "    return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all relevant files from csv / txt\n",
    "doc = codecs.open( config.courses_file[\"file_name\"], 'rU', config.courses_file[\"encoding\"])\n",
    "courses = pd.read_csv(doc, sep = '\\t')\n",
    "\n",
    "doc = codecs.open(  config.product_file[\"file_name\"], 'rU', config.product_file[\"encoding\"])\n",
    "products = pd.read_csv(doc, sep = '\\t')\n",
    "\n",
    "doc = codecs.open( config.product_order_summary_file[\"file_name\"], 'rU', config.product_order_summary_file[\"encoding\"])\n",
    "products_order_summary = pd.read_csv(doc, sep = '\\t')\n",
    "products_order_summary = products_order_summary[pd.notnull(products_order_summary['SkuID'])]\n",
    "products_order_summary = products_order_summary[pd.notnull(products_order_summary['EncryptedRecordID'])]\n",
    "\n",
    "doc = codecs.open( config.user_enrollments_completions_file[\"file_name\"], 'rU', config.user_enrollments_completions_file['encoding'])\n",
    "course_completion = pd.read_csv(doc, sep = '\\t')\n",
    "course_completion = course_completion[pd.notnull(course_completion['CourseCode'])]\n",
    "course_completion = course_completion[pd.notnull(course_completion['EncryptedRecordID'])]\n",
    "\n",
    "doc = codecs.open( config.user_subscriptions_file[\"file_name\"], 'rU', config.user_subscriptions_file[\"encoding\"])\n",
    "user_subscription = pd.read_csv(doc, sep = '\\t')\n",
    "user_subscription = user_subscription[pd.notnull(user_subscription['SkuID'])]\n",
    "user_subscription = user_subscription[pd.notnull(user_subscription['EncryptedUserID'])]\n",
    "\n",
    "demographics = pd.read_csv( config.user__masterlist_file[\"file_name\"], sep = '\\t')\n",
    "\n",
    "# Finding all unique Courses\n",
    "courses_club_unique = courses['CourseCode'].unique()\n",
    "\n",
    "courses_enrolments_club_unique = course_completion['CourseCode'].unique()\n",
    "\n",
    "all_course_club_unique = np.concatenate((courses_club_unique,courses_enrolments_club_unique))\n",
    "all_course_club_unique = np.unique(all_course_club_unique)\n",
    "\n",
    "\n",
    "# Finding all unique Products\n",
    "products_club_unique = products['SkuID'].unique()\n",
    "\n",
    "products_order_club_unique = products_order_summary['SkuID'].unique()\n",
    "\n",
    "all_products_club_unique = np.concatenate((products_club_unique, products_order_club_unique))\n",
    "all_products_club_unique = np.unique(all_products_club_unique)\n",
    "\n",
    "#Finding all unique Subscriptions\n",
    "subscriptions_unique = user_subscription['SkuID'].unique()\n",
    "\n",
    "# Finding all unique Products and products\n",
    "course_products_club_unique = np.concatenate((all_course_club_unique, all_products_club_unique, subscriptions_unique))\n",
    "course_products_club_unique = np.unique(course_products_club_unique)\n",
    "\n",
    "\n",
    "# Finding all unique Users\n",
    "users_from_demographics_unique = demographics['EncryptedRecordID'].unique()\n",
    "\n",
    "users_from_subscription_unique = user_subscription['EncryptedUserID'].unique()\n",
    "\n",
    "users_from_course_completion_unique = course_completion['EncryptedRecordID'].unique()\n",
    "\n",
    "users_from_products_order_summary_unique = products_order_summary['EncryptedRecordID'].unique()\n",
    "\n",
    "users_club = np.concatenate((users_from_demographics_unique,users_from_subscription_unique,users_from_course_completion_unique,users_from_products_order_summary_unique))\n",
    "users_club_unique = np.unique(users_club)\n",
    "\n",
    "\n",
    "# Build a dictionary with the user index for all users\n",
    "d_ui = {}\n",
    "for i in range(len(users_club_unique)):\n",
    "    user = users_club_unique[i]\n",
    "    d_ui[user] = i\n",
    "\n",
    "\n",
    "# Build a dictionary with the course/product index for all course/product\n",
    "d_ci = {}\n",
    "for i in range(len(course_products_club_unique)):\n",
    "    course = course_products_club_unique[i]\n",
    "    d_ci[course] = i\n",
    "\n",
    "# Build a sparse matrix for user-item interaction\n",
    "Mui_tr = sps.lil_matrix((len(users_club_unique), (len(all_course_club_unique) + len(all_products_club_unique))), dtype=np.int8)\n",
    "\n",
    "# Now fill Mui_tr with the info from course_completion file\n",
    "for i in range(len(course_completion)):\n",
    "    user = course_completion[\"EncryptedRecordID\"].values[i]\n",
    "    course = course_completion[\"CourseCode\"].values[i]\n",
    "    ui, ci = d_ui[user], d_ci[course]\n",
    "    Mui_tr[ui, ci] = 1.0\n",
    "\n",
    "# Now fill Mui_tr with the info from products_order_summary file\n",
    "for i in range(len(products_order_summary)):\n",
    "    user = products_order_summary[\"EncryptedRecordID\"].values[i]\n",
    "    course = products_order_summary[\"SkuID\"].values[i]\n",
    "    ui, ci = d_ui[user], d_ci[course]\n",
    "    Mui_tr[ui, ci] = 1.0\n",
    "\n",
    "# Now fill Mui_tr with the info from user_subscription file\n",
    "for i in range(len(user_subscription)):\n",
    "    user = user_subscription[\"EncryptedUserID\"].values[i]\n",
    "    course = user_subscription[\"SkuID\"].values[i]\n",
    "    ui, ci = d_ui[user], d_ci[course]\n",
    "    Mui_tr[ui, ci] = 1.0\n",
    "\n",
    "# Process and find unique fields in user demographics\n",
    "user_unique_cred = process_and_find_unique_fields(demographics, 'Credential')\n",
    "user_unique_expert = process_and_find_unique_fields(demographics, 'Expertise')\n",
    "user_unique_interest = process_and_find_unique_fields(demographics, 'Interest')\n",
    "user_unique_judi = process_and_find_unique_fields(demographics, 'Jurisdiction')\n",
    "user_unique_state = process_and_find_unique_states(demographics, 'State')\n",
    "\n",
    "# d_ui has list of users\n",
    "# Build a dict with the user features in ulist\n",
    "d_uf = {}\n",
    "feature_count = 0\n",
    "for i in range(len(user_unique_cred)):\n",
    "    feature = user_unique_cred[i]\n",
    "    d_uf['%s_credential' %feature] = i\n",
    "\n",
    "feature_count = feature_count + i + 1\n",
    "for i in range(len(user_unique_state)):\n",
    "    feature = user_unique_state[i]\n",
    "    d_uf['%s_state' %feature] = i + feature_count\n",
    "\n",
    "feature_count = feature_count + i + 1\n",
    "for i in range(len(user_unique_expert)):\n",
    "    feature = user_unique_expert[i]\n",
    "    d_uf['%s_expert' %feature] = i + feature_count\n",
    "\n",
    "feature_count = feature_count + i + 1\n",
    "for i in range(len(user_unique_interest)):\n",
    "    feature = user_unique_interest[i]\n",
    "    d_uf['%s_interest' %feature] = i + feature_count\n",
    "\n",
    "feature_count = feature_count + i + 1\n",
    "for i in range(len(user_unique_judi)):\n",
    "    feature = user_unique_judi[i]\n",
    "    d_uf['%s_jurisdiction' %feature] = i + feature_count\n",
    "\n",
    "# d_ui has list of users\n",
    "# Build a sparse matrix for user-metadata\n",
    "Mui_uf = sps.lil_matrix((len(users_club_unique), len(d_uf)), dtype=np.int8)\n",
    "\n",
    "# Now fill Mui_uf with the info from user features\n",
    "for i in range(len(demographics)):\n",
    "    if demographics[\"EncryptedRecordID\"].values[i] == '#Name?':\n",
    "        i = i+1\n",
    "    user = demographics[\"EncryptedRecordID\"].values[i]\n",
    "    ui = d_ui[user]\n",
    "    credential = demographics['Credential'].values[i]\n",
    "    credential = str(credential).replace(' ', '')\n",
    "    credential = str(credential).replace('\\\\', ',')\n",
    "    fields = str(credential).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "            uf = d_uf[fields[j].lower() + '_credential']\n",
    "            Mui_uf[ui, uf] = 1.0\n",
    "\n",
    "\n",
    "    state = demographics['State'].values[i]\n",
    "    state = str(state).replace(' ', '')\n",
    "    state = str(state).replace('\\\\', ',')\n",
    "    fields = str(state).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan' and (not ('.' in fields[j])):\n",
    "            uf = d_uf[fields[j].lower() + '_state']\n",
    "            Mui_uf[ui, uf] = 1.0\n",
    "\n",
    "    expert = demographics['Expertise'].values[i]\n",
    "    expert = str(expert).replace(' ', '')\n",
    "    expert = str(expert).replace('\\\\', ',')\n",
    "    fields = str(expert).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "            uf = d_uf[fields[j].lower() + '_expert']\n",
    "            Mui_uf[ui, uf] = 1.0\n",
    "\n",
    "    interest = demographics['Interest'].values[i]\n",
    "    interest = str(interest).replace(' ', '')\n",
    "    interest = str(interest).replace('\\\\', ',')\n",
    "    fields = str(interest).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "            uf = d_uf[fields[j].lower() + '_interest']\n",
    "            Mui_uf[ui, uf] = 1.0\n",
    "\n",
    "    jurisdiction = demographics['Jurisdiction'].values[i]\n",
    "    jurisdiction = str(jurisdiction).replace(' ', '')\n",
    "    jurisdiction = str(jurisdiction).replace('\\\\', ',')\n",
    "    fields = str(jurisdiction).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "            uf = d_uf[fields[j].lower() + '_jurisdiction']\n",
    "            Mui_uf[ui, uf] = 1.0\n",
    "\n",
    "Mui_uf = hstack([Mui_uf, identity(len(users_club_unique))])\n",
    "\n",
    "required_columns = ['CourseCode', 'AllFieldOfStudy', 'TotalCreditHours', 'Topic', 'Type', 'PriceRange', 'Keywords', 'EventEndDate', 'EventStartDate']\n",
    "courses = pd.DataFrame(courses, columns=required_columns)\n",
    "\n",
    "# Process and find unique fields in Courses\n",
    "course_unique_FOS = process_and_find_unique_fields(courses, 'AllFieldOfStudy')\n",
    "\n",
    "set_topics = []\n",
    "course_topic = courses['Topic'].unique()\n",
    "for i in range(0, len(course_topic)):\n",
    "    set_topics.append([x.strip() for x in (str((course_topic[i]))).split('|')])\n",
    "course_unique_topics = []\n",
    "for topics in course_topic:\n",
    "    topic = str(topics).split('|')\n",
    "    for top in topic:\n",
    "        if top != '' and top !='None' and top !='nan':\n",
    "            course_unique_topics.append(str(top))\n",
    "\n",
    "course_unique_topics = list(set(course_unique_topics))\n",
    "\n",
    "course_unique_type = list(courses['Type'].unique())\n",
    "course_unique_type = [x for x in course_unique_type if str(x) != 'nan']\n",
    "\n",
    "course_unique_keywords = get_top_Keywords(courses['Keywords'].unique(), 100)\n",
    "\n",
    "course_price = courses['PriceRange'].unique()\n",
    "average_course_price_unique =[]\n",
    "for i in range(0, len(course_price)):\n",
    "    if not (pd.isnull(course_price[i])):\n",
    "        priceends = course_price[i].split(' - ')\n",
    "        for j in range(0, len(priceends)):\n",
    "            priceends[j] = float(priceends[j].replace('$', '').replace(',',''))\n",
    "        average_course_price_unique.append(int(np.mean(priceends)))\n",
    "\n",
    "course_bins = np.linspace(0, max(average_course_price_unique), 10)\n",
    "\n",
    "required_columns = ['ProductCode', 'SkuID', 'Type', 'AllFieldOfStudy', 'TotalCreditHours', 'PriceRange', 'Topic', 'Keywords']\n",
    "products = pd.DataFrame(products, columns=required_columns)\n",
    "\n",
    "# Process and find unique fields in Products\n",
    "product_unique_type = list(products['Type'].unique())\n",
    "product_unique_type = [x for x in product_unique_type if str(x) != 'nan']\n",
    "\n",
    "product_unique_FOS = process_and_find_unique_fields(products, 'AllFieldOfStudy')\n",
    "len(product_unique_FOS)#36\n",
    "\n",
    "set_topics = []\n",
    "product_topic = products['Topic'].unique()\n",
    "for i in range(0, len(product_topic)):\n",
    "    set_topics.append([x.strip() for x in (str((product_topic[i]))).split('|')])\n",
    "product_unique_topics = []\n",
    "for topics in product_topic:\n",
    "    topic = str(topics).split('|')\n",
    "    for top in topic:\n",
    "        if top != '' and top !='None' and top !='nan':\n",
    "            product_unique_topics.append(str(top))\n",
    "\n",
    "product_unique_topics = list(set(product_unique_topics))\n",
    "\n",
    "product_unique_keywords = get_top_Keywords(products['Keywords'].unique(), 100)\n",
    "\n",
    "product_price = products['PriceRange'].unique()\n",
    "average_product_price_unique =[]\n",
    "for i in range(0, len(product_price)):\n",
    "    if not (pd.isnull(product_price[i])):\n",
    "        priceends = product_price[i].split(' - ')\n",
    "        for j in range(0, len(priceends)):\n",
    "            priceends[j] = float(priceends[j].replace('$', '').replace(',',''))\n",
    "        average_product_price_unique.append(int(np.mean(priceends)))\n",
    "\n",
    "product_bins = np.linspace(0, max(average_product_price_unique), 10)\n",
    "\n",
    "#d_ci has list of courses/products\n",
    "# Build a dictionary with the item features\n",
    "d_if = {}\n",
    "unique_type = list(set().union(product_unique_type,course_unique_type))\n",
    "feature_count = 0\n",
    "for i in range(len(unique_type)):\n",
    "    feature = unique_type[i]\n",
    "    d_if['%s_type' %feature] = i\n",
    "\n",
    "unique_FOS = list(set().union(product_unique_FOS,course_unique_FOS))\n",
    "feature_count = feature_count + i + 1\n",
    "for i in range(len(unique_FOS)):\n",
    "    feature = unique_FOS[i]\n",
    "    d_if['%s_FOS' %feature] = i + feature_count\n",
    "\n",
    "feature_count = feature_count + i + 1\n",
    "d_if['TotalCreditHours'] = feature_count\n",
    "\n",
    "feature_count = feature_count + 1\n",
    "unique_topic = list(set().union(product_unique_topics,course_unique_topics))\n",
    "for i in range(len(unique_topic)):\n",
    "    feature = unique_topic[i]\n",
    "    d_if['%s_topic' %feature] = i + feature_count\n",
    "\n",
    "unique_keywords = list(set().union(product_unique_keywords,course_unique_keywords))\n",
    "feature_count = feature_count + i + 1\n",
    "\n",
    "for i in range(len(unique_keywords)):\n",
    "    feature = unique_keywords[i]\n",
    "    d_if['%s_keyword' %feature] = i + feature_count\n",
    "\n",
    "price_range = max(max(average_product_price_unique), max(average_course_price_unique))\n",
    "price_bins = np.linspace(0, price_range, 10)\n",
    "feature_count = feature_count + i + 1\n",
    "\n",
    "for i in range(len(product_bins)):\n",
    "    d_if['price Range %s' %str(i+1)] = i + feature_count\n",
    "feature_count = feature_count + i + 1\n",
    "\n",
    "# Build the user x item matrices using scipy lil_matrix\n",
    "Mui_if = sps.lil_matrix((len(course_products_club_unique), len(d_if)), dtype=np.int8)\n",
    "\n",
    "# Now fill Mui_tr with the info from course features\n",
    "for i in range(len(courses)):\n",
    "    course = courses[\"CourseCode\"].values[i]\n",
    "    ci = d_ci[course]\n",
    "    FOS = courses['AllFieldOfStudy'].values[i]\n",
    "    FOS = str(FOS).replace(' ', '')\n",
    "    FOS = str(FOS).replace('\\\\', ',')\n",
    "    FOS = str(FOS).replace(';', ',')\n",
    "    fields = str(FOS).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "            fi = d_if[fields[j].lower() + '_FOS']\n",
    "            Mui_if[ci, fi] = 1.0\n",
    "\n",
    "    credit_hours = courses[\"TotalCreditHours\"].values[i]\n",
    "    if not np.isnan(credit_hours):\n",
    "        fi = d_if['TotalCreditHours']\n",
    "        Mui_if[ci, fi] = credit_hours\n",
    "\n",
    "    typec = courses['Type'].values[i] + '_type'\n",
    "    fi = d_if[typec]\n",
    "    Mui_if[ci, fi] = 1.0\n",
    "\n",
    "    pricerange = courses['PriceRange'].values[i]\n",
    "    if (not (pd.isnull(pricerange))):\n",
    "        priceends = str(pricerange).split(' - ')\n",
    "        for j in range(0, len(priceends)):\n",
    "            priceends[j] = float(priceends[j].replace('$', '').replace(',',''))\n",
    "        price_avg = int(np.mean(priceends))\n",
    "        course_price_bin = int(np.digitize(price_avg, price_bins))\n",
    "        pricerange = 'price Range %s' %course_price_bin\n",
    "        fi = d_if[pricerange]\n",
    "        Mui_if[ci, fi] = 1.0\n",
    "\n",
    "    topic = str(courses['Topic'].values[0]).split('|')\n",
    "    for top in topic:\n",
    "        if top != '' and top !='None' and top !='nan':\n",
    "            fi = d_if[str(top) + '_topic']\n",
    "            Mui_if[ci, fi] = 1.0\n",
    "\n",
    "# Now fill Mui_tr with the info from product features\n",
    "for i in range(len(products)):\n",
    "    product = products[\"SkuID\"].values[i]\n",
    "    ci = d_ci[product]\n",
    "    FOS = products['AllFieldOfStudy'].values[i]\n",
    "    FOS = str(FOS).replace(' ', '')\n",
    "    FOS = str(FOS).replace('\\\\', ',')\n",
    "    FOS = str(FOS).replace(';', ',')\n",
    "    fields = str(FOS).split(',')\n",
    "    for j in range(0, len(fields)):\n",
    "        if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "            fi = d_if[fields[j].lower() + '_FOS']\n",
    "            Mui_if[ci, fi] = 1.0\n",
    "\n",
    "    credit_hours = products[\"TotalCreditHours\"].values[i]\n",
    "    if not np.isnan(credit_hours):\n",
    "        fi = d_if['TotalCreditHours']\n",
    "        Mui_if[ci, fi] = credit_hours\n",
    "\n",
    "    typec = products['Type'].values[i]\n",
    "    if str(typec) != 'nan':\n",
    "        typec = typec + '_type'\n",
    "        fi = d_if[typec]\n",
    "        Mui_if[ci, fi] = 1.0\n",
    "\n",
    "    pricerange = products['PriceRange'].values[i]\n",
    "    if (not (pd.isnull(pricerange))):\n",
    "        priceends = str(pricerange).split(' - ')\n",
    "        for j in range(0, len(priceends)):\n",
    "            priceends[j] = float(priceends[j].replace('$', '').replace(',',''))\n",
    "        price_avg = int(np.mean(priceends))\n",
    "        product_price_bin = int(np.digitize(price_avg, price_bins))\n",
    "        pricerange = 'price Range %s' %product_price_bin\n",
    "        fi = d_if[pricerange]\n",
    "        Mui_if[ci, fi] = 1.0\n",
    "\n",
    "    topic = str(products['Topic'].values[0]).split('|')\n",
    "    for top in topic:\n",
    "        if top != '' and top !='None' and top !='nan':\n",
    "            fi = d_if[str(top) + '_topic']\n",
    "            Mui_if[ci, fi] = 1.0\n",
    "\n",
    "Mui_if = hstack([Mui_if, identity(len(course_products_club_unique))])\n",
    "\n",
    "# Filtering out courses with metadata, since only they can be recommended\n",
    "courses_with_metadata_index = []\n",
    "courses_with_metadata = []\n",
    "for i in range(len(all_course_club_unique)):\n",
    "    if any(courses.CourseCode == all_course_club_unique[i]):\n",
    "        courses_with_metadata.append(all_course_club_unique[i])\n",
    "        courses_with_metadata_index.append(d_ci[all_course_club_unique[i]])\n",
    "\n",
    "courses_with_metadata = np.array(courses_with_metadata)\n",
    "\n",
    "# Filtering out products with metadata, since only they can be recommended\n",
    "products_with_metadata_index = []\n",
    "products_with_metadata = []\n",
    "for i in range(len(all_products_club_unique)):\n",
    "    if any(products.SkuID == all_products_club_unique[i]):\n",
    "        products_with_metadata.append(all_products_club_unique[i])\n",
    "        products_with_metadata_index.append(d_ci[all_products_club_unique[i]])\n",
    "\n",
    "products_with_metadata = np.array(products_with_metadata)\n",
    "\n",
    "dictionary_courses_with_metadata = {}\n",
    "for item in courses_with_metadata:\n",
    "        item_info = courses.loc[courses['CourseCode'] == item]\n",
    "        if len(item_info) > 0:\n",
    "            FOS = item_info.iloc[-1]['AllFieldOfStudy']\n",
    "            FOS_array = []\n",
    "            if FOS != np.nan and FOS != 'None':    \n",
    "                FOS = str(FOS).replace(';', ',')\n",
    "                fields = str(FOS).split(', ')\n",
    "                for j in range(0, len(fields)):\n",
    "                    if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "                        FOS_array.append(fields[j])\n",
    "            try:\n",
    "                start_date = datetime.strptime(item_info.iloc[-1]['EventStartDate'], '%Y-%m-%d %H:%M').isoformat()\n",
    "            except:\n",
    "                start_date = None\n",
    "            try:\n",
    "                end_date = datetime.strptime(item_info.iloc[-1]['EventStartDate'], '%Y-%m-%d %H:%M').isoformat()\n",
    "            except:\n",
    "                end_date = None\n",
    "            dictionary_courses_with_metadata[item] = {'itemId' : item,\n",
    "                                                   'Type' : item_info.iloc[-1]['Type'],\n",
    "                                                   'AllFieldOfStudy' : FOS_array,\n",
    "                                                   'EventEndDate' : end_date,\n",
    "                                                   'EventStartDate' : start_date}\n",
    "\n",
    "\n",
    "dictionary_products_with_metadata = {}\n",
    "for item in products_with_metadata:\n",
    "        item_info = products.loc[products['SkuID'] == item]\n",
    "        len_item_info = len(item_info)\n",
    "        if len_item_info > 0:\n",
    "            FOS = item_info.iloc[-1]['AllFieldOfStudy']\n",
    "            FOS_array = []\n",
    "            if FOS != np.nan and FOS != 'None':    \n",
    "                FOS = str(FOS).replace(';', ',')\n",
    "                fields = str(FOS).split(', ')\n",
    "                for j in range(0, len(fields)):\n",
    "                    if fields[j] !='' and fields[j] !='None'and fields[j] !='nan':\n",
    "                        FOS_array.append(fields[j])\n",
    "            try:\n",
    "                start_date = datetime.strptime(item_info.iloc[-1]['EventStartDate'], '%Y-%m-%d %H:%M').isoformat()\n",
    "            except:\n",
    "                start_date = None\n",
    "            try:\n",
    "                end_date = datetime.strptime(item_info.iloc[-1]['EventStartDate'], '%Y-%m-%d %H:%M').isoformat()\n",
    "            except:\n",
    "                end_date = None\n",
    "            dictionary_products_with_metadata[item] ={'itemId' : item_info.iloc[-1]['ProductCode'],\n",
    "                                                       'Type' : item_info.iloc[-1]['Type'],\n",
    "                                                       'AllFieldOfStudy' : FOS_array,\n",
    "                                                       'EventEndDate' : end_date,\n",
    "                                                       'EventStartDate' : start_date}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def make_train(ratings, pct_test = 0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix. \n",
    "    \n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the \n",
    "    training set for later comparison to the test set, which contains all of the original ratings. \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs \n",
    "    that originally had interaction set back to zero.\n",
    "    \n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order \n",
    "    compares with the actual interactions.\n",
    "    \n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(1) # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    \n",
    "    test_set = sps.lil_matrix((len(users_club_unique), (len(all_course_club_unique) + len(all_products_club_unique))), dtype=np.int8)\n",
    "\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    test_set[user_inds, item_inds] = 1.0\n",
    "    training_set = sps.coo_matrix(training_set)\n",
    "    #training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, user_indices = make_train(Mui_tr,pct_test = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import forest_minimize\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score, precision_at_k\n",
    "import lightfm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_wsideinfo(params):\n",
    "    # unpack\n",
    "    epochs, learning_rate,\\\n",
    "    no_components, item_alpha,\\\n",
    "    scale = params\n",
    "    \n",
    "    user_alpha = item_alpha * scale\n",
    "    model = LightFM(loss='warp',\n",
    "                    random_state=2016,\n",
    "                    learning_rate=learning_rate,\n",
    "                    no_components=no_components,\n",
    "                    user_alpha=user_alpha,\n",
    "                    item_alpha=item_alpha)\n",
    "    model.fit( train,\n",
    "          epochs=epochs,\n",
    "          num_threads=4, verbose=True)\n",
    "    \n",
    "    patks = precision_at_k(model,  test,  \n",
    "                           k=10, num_threads=4)\n",
    "    mapatk = np.mean(patks)\n",
    "    # Make negative because we want to _minimize_ objective\n",
    "    out = -mapatk\n",
    "    \n",
    "    pickle.dump(model,open((\"model%s.pickle\" %epochs),\"wb\"))\n",
    "    print 'epochs:', epochs\n",
    "    print 'learning_rate:', learning_rate\n",
    "    print 'no_components:', no_components\n",
    "    print 'item_alpha:', item_alpha\n",
    "    print 'scale:', scale\n",
    "    \n",
    "    # Weird shit going on\n",
    "    if np.abs(out + 1) < 0.01 or out < -1.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d9f31922a219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m res_fm_itemfeat = forest_minimize(objective_wsideinfo, space, n_calls=10,\n\u001b[1;32m     11\u001b[0m                                   \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                   verbose=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/skopt/optimizer/forest.pyc\u001b[0m in \u001b[0;36mforest_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, x0, y0, random_state, verbose, callback, n_points, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    160\u001b[0m                          \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macq_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                          \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                          callback=callback, acq_optimizer=\"sampling\")\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/skopt/optimizer/base.pyc\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# no need to fit a model on the last iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_calls\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f54c4f1b5dd0>\u001b[0m in \u001b[0;36mobjective_wsideinfo\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.fit( train,\n\u001b[1;32m     13\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           num_threads=4, verbose=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     patks = precision_at_k(model,  test,  \n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/lightfm/lightfm.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, interactions, user_features, item_features, sample_weight, epochs, num_threads, verbose)\u001b[0m\n\u001b[1;32m    409\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                                 \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                                 verbose=verbose)\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     def fit_partial(self, interactions,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/lightfm/lightfm.pyc\u001b[0m in \u001b[0;36mfit_partial\u001b[0;34m(self, interactions, user_features, item_features, sample_weight, epochs, num_threads, verbose)\u001b[0m\n\u001b[1;32m    483\u001b[0m             self._initialize(self.no_components,\n\u001b[1;32m    484\u001b[0m                              \u001b[0mitem_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                              user_features.shape[1])\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# Check that the dimensionality of the feature matrices has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/lightfm/lightfm.pyc\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, no_components, no_item_features, no_user_features)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# Initialise user features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         self.user_embeddings = (\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_user_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_components\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             no_components).astype(np.float32)\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_embedding_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.rand\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.random_sample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont0_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "space = [(10, 150), # epochs\n",
    "         (0.007, 0.07, 'log-uniform'), # learning_rate\n",
    "         (20, 200), # no_components\n",
    "         (10**-5, 10**-3, 'log-uniform'), # item_alpha\n",
    "         (0.001, 1., 'log-uniform') # user_scaling\n",
    "        ]\n",
    "#x0 = res_fm.x.append(1.)\n",
    "# This typecast is required\n",
    "item_features = Mui_if.astype(np.float32)\n",
    "res_fm_itemfeat = forest_minimize(objective_wsideinfo, space, n_calls=10,\n",
    "                                  random_state=24,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_courses_for_user(user, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = -1):\n",
    "    # Get index of user from user dictionary\n",
    "    try:\n",
    "        user_id = d_ui[user]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error('User ID does not exist')\n",
    "        raise Exception(\"User ID does not exist\")\n",
    "    \n",
    "    # Predict Scores for all courses for this user and store in dictionary\n",
    "    scores = model.predict(user_id, np.array(courses_with_metadata_index))\n",
    "    d_scores = {}\n",
    "    for i in range(len(scores)):\n",
    "        score = scores[i]\n",
    "        d_scores[courses_with_metadata[i]] = score\n",
    "        \n",
    "    # Sorting scores in order\n",
    "    top_items = courses_with_metadata[np.argsort(-scores)]\n",
    "    \n",
    "    # Iterating through all courses and populating the list of top N items and their metadata\n",
    "    top_N_list = []\n",
    "    counter = 0\n",
    "    print 'Courses'\n",
    "    for item in top_items:\n",
    "        not_courses = user_course_int.loc[user_course_int['CourseCode'] == item]\n",
    "        if len(not_courses) == 0:\n",
    "            print str(counter+1) + ': ' + str(item)\n",
    "            title_db = courses.loc[courses['CourseCode'] == item]\n",
    "            print title_db.iloc[-1]['Title']\n",
    "            print '-'\n",
    "            counter =counter + 1\n",
    "            \n",
    "        if (top_N != -1) and (counter == top_N):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_products_for_user(user, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = -1):\n",
    "    # Get index of user from user dictionary\n",
    "    try:\n",
    "        user_id = d_ui[user]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error('User ID does not exist')\n",
    "        raise Exception(\"User ID does not exist\")\n",
    "    \n",
    "    # Predict Scores for all products for this user and store in dictionary\n",
    "    scores = model.predict(user_id, np.array(products_with_metadata_index))\n",
    "    d_scores = {}\n",
    "    for i in range(len(scores)):\n",
    "        score = scores[i]\n",
    "        d_scores[products_with_metadata[i]] = score\n",
    "    top_items = products_with_metadata[np.argsort(-scores)]\n",
    "    \n",
    "    # Iterating through all products and populating the list of top N items and their metadata\n",
    "    top_N_list = []\n",
    "    counter = 0\n",
    "    print 'Products'\n",
    "    for item in top_items:\n",
    "        not_products = user_product_int.loc[user_product_int['SkuID'] == item]\n",
    "        if len(not_products) == 0:\n",
    "            prodwithsku = products.loc[products['SkuID'] == item]\n",
    "            prodcode = prodwithsku.iloc[-1]['ProductCode']\n",
    "            prodtit = prodwithsku.iloc[-1]['Title']\n",
    "            print str(counter+1) + ': ' + prodcode\n",
    "            print prodtit\n",
    "            print '-'\n",
    "            counter =counter + 1\n",
    "            if (counter == top_N) and (top_N != -1):\n",
    "                break\n",
    "    return top_N_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = codecs.open( config.courses_file[\"file_name\"], 'rU', config.courses_file[\"encoding\"])\n",
    "courses = pd.read_csv(doc, sep = '\\t')\n",
    "doc = codecs.open(  config.product_file[\"file_name\"], 'rU', config.product_file[\"encoding\"])\n",
    "products = pd.read_csv(doc, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'zfePq8N2N87F2UYqWnWGp8bjCVTchxkV1VDVT3A7IBY='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'gdf02qFth7wzK6B3wuXPzg=='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'Fl24VZQE1yXsPxgOTzav0vf8ri5gmdjowE5L0miCg18='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'gD9WJy7hzwuj4RYNGbqtrQASW7a6Xl4m4ejqyakIuCw='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'tU0Kx3IWl3Zk2IPVD97kAna8+8Y5D0xun9H3uI9zikg='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'JOv+s7BwRpeMM2k3M6beYDTpnObMsHjmO9fgNtnHhTw='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'sj46lvYsHdxsd1LYZEymsHxMLwToEbijyfVLdGgKbgE='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = 'L2AwjZ0dAAw+IxGoiinennSISwNBtCBGksevpUhcVWk='\n",
    "user_course_int = course_completion.loc[course_completion['EncryptedRecordID'] == userid]\n",
    "user_product_int = products_order_summary.loc[products_order_summary['EncryptedRecordID'] == userid]\n",
    "\n",
    "predict_courses_for_user(userid, model, d_ui, courses_with_metadata_index, courses_with_metadata, dictionary_courses_with_metadata, user_course_int,courses, top_N = 10)\n",
    "predict_products_for_user(userid, model, d_ui, products_with_metadata_index, products_with_metadata, dictionary_products_with_metadata,user_product_int,products, top_N = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1\n",
    "Maximimum p@k found: 0.19531\n",
    "\n",
    "Optimal parameters:\n",
    "\n",
    "epochs: 68\n",
    "\n",
    "learning_rate: 0.0271016262622\n",
    "\n",
    "no_components: 59\n",
    "\n",
    "item_alpha: 0.000114232541556\n",
    "\n",
    "scaling: 0.0505980387466\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2\n",
    "epochs: 36\n",
    "\n",
    "learning_rate: 0.0303690277085\n",
    "\n",
    "no_components: 88\n",
    "\n",
    "item_alpha: 0.000154468627875\n",
    "\n",
    "scale: 0.0192719675083\n",
    "\n",
    "Time taken: 2331.9187\n",
    "\n",
    "Function value obtained: -0.2019\n",
    "\n",
    "\n",
    "#### 3\n",
    "epochs: 28\n",
    "\n",
    "learning_rate: 0.0386129433916\n",
    "\n",
    "no_components: 122\n",
    "\n",
    "item_alpha: 7.88850830743e-05\n",
    "\n",
    "scale: 0.00890053497733\n",
    "\n",
    "Iteration No: 3 ended. Evaluation done at random point.\n",
    "\n",
    "Time taken: 3023.3377\n",
    "\n",
    "Function value obtained: -0.2833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration No: 1 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "epochs: 36\n",
    "learning_rate: 0.0303690277085\n",
    "no_components: 88\n",
    "item_alpha: 0.000154468627875\n",
    "scale: 0.0192719675083\n",
    "Iteration No: 1 ended. Evaluation done at random point.\n",
    "Time taken: 2331.9187\n",
    "Function value obtained: -0.2019\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 2 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "epochs: 99\n",
    "learning_rate: 0.291649845477\n",
    "no_components: 109\n",
    "item_alpha: 7.4399773602e-05\n",
    "scale: 0.151736030516\n",
    "Iteration No: 2 ended. Evaluation done at random point.\n",
    "Time taken: 5087.5951\n",
    "Function value obtained: -0.0001\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 3 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "epochs: 11\n",
    "learning_rate: 0.881877226297\n",
    "no_components: 175\n",
    "item_alpha: 0.00017740928056\n",
    "scale: 0.00949837261602\n",
    "Iteration No: 3 ended. Evaluation done at random point.\n",
    "Time taken: 4206.9628\n",
    "Function value obtained: -0.0001\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 4 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "epochs: 33\n",
    "learning_rate: 0.00814854262879\n",
    "no_components: 27\n",
    "item_alpha: 1.76434940795e-05\n",
    "scale: 0.00364469921407\n",
    "Iteration No: 4 ended. Evaluation done at random point.\n",
    "Time taken: 1087.3056\n",
    "Function value obtained: -0.0859\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 5 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "Epoch 99\n",
    "Epoch 100\n",
    "Epoch 101\n",
    "Epoch 102\n",
    "Epoch 103\n",
    "Epoch 104\n",
    "Epoch 105\n",
    "Epoch 106\n",
    "Epoch 107\n",
    "Epoch 108\n",
    "Epoch 109\n",
    "Epoch 110\n",
    "Epoch 111\n",
    "Epoch 112\n",
    "Epoch 113\n",
    "Epoch 114\n",
    "Epoch 115\n",
    "epochs: 116\n",
    "learning_rate: 0.00815902860519\n",
    "no_components: 179\n",
    "item_alpha: 0.000448849033535\n",
    "scale: 0.0358901605615\n",
    "Iteration No: 5 ended. Evaluation done at random point.\n",
    "Time taken: 7807.9910\n",
    "Function value obtained: -0.1026\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 6 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "epochs: 71\n",
    "learning_rate: 0.180752553867\n",
    "no_components: 105\n",
    "item_alpha: 7.94637412819e-05\n",
    "scale: 0.109766783631\n",
    "Iteration No: 6 ended. Evaluation done at random point.\n",
    "Time taken: 4145.5749\n",
    "Function value obtained: -0.0001\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 7 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "epochs: 65\n",
    "learning_rate: 0.748598548823\n",
    "no_components: 23\n",
    "item_alpha: 0.000186140771089\n",
    "scale: 0.00998551452033\n",
    "Iteration No: 7 ended. Evaluation done at random point.\n",
    "Time taken: 1174.1516\n",
    "Function value obtained: -0.0000\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 8 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "Epoch 99\n",
    "epochs: 100\n",
    "learning_rate: 0.188843150862\n",
    "no_components: 119\n",
    "item_alpha: 4.55718026116e-05\n",
    "scale: 0.00120050686884\n",
    "Iteration No: 8 ended. Evaluation done at random point.\n",
    "Time taken: 5294.4158\n",
    "Function value obtained: -0.0001\n",
    "Current minimum: -0.2019\n",
    "Iteration No: 9 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teration No: 1 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "Epoch 99\n",
    "epochs: 100\n",
    "learning_rate: 0.00139979092292\n",
    "no_components: 37\n",
    "item_alpha: 0.000215388838627\n",
    "scale: 0.0476021765235\n",
    "Iteration No: 1 ended. Evaluation done at random point.\n",
    "Time taken: 1916.6598\n",
    "Function value obtained: -0.0505\n",
    "Current minimum: -0.0505\n",
    "Iteration No: 2 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "epochs: 81\n",
    "learning_rate: 0.00713574903019\n",
    "no_components: 37\n",
    "item_alpha: 5.38238622108e-05\n",
    "scale: 0.00283309916395\n",
    "Iteration No: 2 ended. Evaluation done at random point.\n",
    "Time taken: 1663.8152\n",
    "Function value obtained: -0.0948\n",
    "Current minimum: -0.0948\n",
    "Iteration No: 3 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "epochs: 28\n",
    "learning_rate: 0.0227398409163\n",
    "no_components: 122\n",
    "item_alpha: 7.88850830743e-05\n",
    "scale: 0.00890053497733\n",
    "Iteration No: 3 ended. Evaluation done at random point.\n",
    "Time taken: 3054.0798\n",
    "Function value obtained: -0.1947\n",
    "Current minimum: -0.1947\n",
    "Iteration No: 4 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "Epoch 99\n",
    "Epoch 100\n",
    "Epoch 101\n",
    "Epoch 102\n",
    "Epoch 103\n",
    "Epoch 104\n",
    "Epoch 105\n",
    "Epoch 106\n",
    "Epoch 107\n",
    "Epoch 108\n",
    "Epoch 109\n",
    "Epoch 110\n",
    "Epoch 111\n",
    "Epoch 112\n",
    "Epoch 113\n",
    "Epoch 114\n",
    "Epoch 115\n",
    "Epoch 116\n",
    "Epoch 117\n",
    "Epoch 118\n",
    "Epoch 119\n",
    "Epoch 120\n",
    "Epoch 121\n",
    "Epoch 122\n",
    "Epoch 123\n",
    "Epoch 124\n",
    "Epoch 125\n",
    "Epoch 126\n",
    "Epoch 127\n",
    "Epoch 128\n",
    "Epoch 129\n",
    "Epoch 130\n",
    "Epoch 131\n",
    "Epoch 132\n",
    "Epoch 133\n",
    "Epoch 134\n",
    "Epoch 135\n",
    "Epoch 136\n",
    "Epoch 137\n",
    "Epoch 138\n",
    "Epoch 139\n",
    "Epoch 140\n",
    "Epoch 141\n",
    "Epoch 142\n",
    "Epoch 143\n",
    "Epoch 144\n",
    "Epoch 145\n",
    "Epoch 146\n",
    "Epoch 147\n",
    "epochs: 148\n",
    "learning_rate: 0.00128309838443\n",
    "no_components: 129\n",
    "item_alpha: 0.00067691810234\n",
    "scale: 0.023981466838\n",
    "Iteration No: 4 ended. Evaluation done at random point.\n",
    "Time taken: 6815.9836\n",
    "Function value obtained: -0.0503\n",
    "Current minimum: -0.1947\n",
    "Iteration No: 5 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "Epoch 99\n",
    "Epoch 100\n",
    "Epoch 101\n",
    "Epoch 102\n",
    "Epoch 103\n",
    "Epoch 104\n",
    "Epoch 105\n",
    "Epoch 106\n",
    "Epoch 107\n",
    "Epoch 108\n",
    "Epoch 109\n",
    "Epoch 110\n",
    "Epoch 111\n",
    "Epoch 112\n",
    "Epoch 113\n",
    "Epoch 114\n",
    "Epoch 115\n",
    "Epoch 116\n",
    "Epoch 117\n",
    "Epoch 118\n",
    "epochs: 119\n",
    "learning_rate: 0.00131601711594\n",
    "no_components: 80\n",
    "item_alpha: 8.29973918428e-05\n",
    "scale: 0.0430302262756\n",
    "Iteration No: 5 ended. Evaluation done at random point.\n",
    "Time taken: 4204.7592\n",
    "Function value obtained: -0.0654\n",
    "Current minimum: -0.1947\n",
    "Iteration No: 6 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90\n",
    "Epoch 91\n",
    "Epoch 92\n",
    "Epoch 93\n",
    "Epoch 94\n",
    "Epoch 95\n",
    "Epoch 96\n",
    "Epoch 97\n",
    "Epoch 98\n",
    "Epoch 99\n",
    "Epoch 100\n",
    "Epoch 101\n",
    "Epoch 102\n",
    "Epoch 103\n",
    "Epoch 104\n",
    "Epoch 105\n",
    "Epoch 106\n",
    "Epoch 107\n",
    "Epoch 108\n",
    "Epoch 109\n",
    "Epoch 110\n",
    "Epoch 111\n",
    "Epoch 112\n",
    "Epoch 113\n",
    "Epoch 114\n",
    "Epoch 115\n",
    "Epoch 116\n",
    "Epoch 117\n",
    "Epoch 118\n",
    "Epoch 119\n",
    "Epoch 120\n",
    "Epoch 121\n",
    "Epoch 122\n",
    "Epoch 123\n",
    "Epoch 124\n",
    "Epoch 125\n",
    "Epoch 126\n",
    "Epoch 127\n",
    "Epoch 128\n",
    "Epoch 129\n",
    "Epoch 130\n",
    "Epoch 131\n",
    "Epoch 132\n",
    "epochs: 133\n",
    "learning_rate: 0.0174872294366\n",
    "no_components: 35\n",
    "item_alpha: 0.000808195938312\n",
    "scale: 0.0512699765082\n",
    "Iteration No: 6 ended. Evaluation done at random point.\n",
    "Time taken: 2884.1922\n",
    "Function value obtained: -0.0922\n",
    "Current minimum: -0.1947\n",
    "Iteration No: 7 started. Evaluating function at random point.\n",
    "Epoch 0\n",
    "Epoch 1\n",
    "Epoch 2\n",
    "Epoch 3\n",
    "Epoch 4\n",
    "Epoch 5\n",
    "Epoch 6\n",
    "Epoch 7\n",
    "Epoch 8\n",
    "Epoch 9\n",
    "Epoch 10\n",
    "Epoch 11\n",
    "Epoch 12\n",
    "Epoch 13\n",
    "Epoch 14\n",
    "Epoch 15\n",
    "Epoch 16\n",
    "Epoch 17\n",
    "Epoch 18\n",
    "Epoch 19\n",
    "Epoch 20\n",
    "Epoch 21\n",
    "Epoch 22\n",
    "Epoch 23\n",
    "Epoch 24\n",
    "Epoch 25\n",
    "Epoch 26\n",
    "Epoch 27\n",
    "Epoch 28\n",
    "Epoch 29\n",
    "Epoch 30\n",
    "Epoch 31\n",
    "Epoch 32\n",
    "Epoch 33\n",
    "Epoch 34\n",
    "Epoch 35\n",
    "Epoch 36\n",
    "Epoch 37\n",
    "Epoch 38\n",
    "Epoch 39\n",
    "Epoch 40\n",
    "Epoch 41\n",
    "Epoch 42\n",
    "Epoch 43\n",
    "Epoch 44\n",
    "Epoch 45\n",
    "Epoch 46\n",
    "Epoch 47\n",
    "Epoch 48\n",
    "Epoch 49\n",
    "Epoch 50\n",
    "Epoch 51\n",
    "Epoch 52\n",
    "Epoch 53\n",
    "Epoch 54\n",
    "Epoch 55\n",
    "Epoch 56\n",
    "Epoch 57\n",
    "Epoch 58\n",
    "Epoch 59\n",
    "Epoch 60\n",
    "Epoch 61\n",
    "Epoch 62\n",
    "Epoch 63\n",
    "Epoch 64\n",
    "Epoch 65\n",
    "Epoch 66\n",
    "Epoch 67\n",
    "Epoch 68\n",
    "Epoch 69\n",
    "Epoch 70\n",
    "Epoch 71\n",
    "Epoch 72\n",
    "Epoch 73\n",
    "Epoch 74\n",
    "Epoch 75\n",
    "Epoch 76\n",
    "Epoch 77\n",
    "Epoch 78\n",
    "Epoch 79\n",
    "Epoch 80\n",
    "Epoch 81\n",
    "Epoch 82\n",
    "Epoch 83\n",
    "Epoch 84\n",
    "Epoch 85\n",
    "Epoch 86\n",
    "Epoch 87\n",
    "Epoch 88\n",
    "Epoch 89\n",
    "Epoch 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Mui_tr[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
